[#publish-website]
== Publish website

We now copy the results of the Antora website generation over to the `causeway-site` repo:

* in the `causeway-site` repo, check out the `asf-site` branch:
+
[source,bash,subs="attributes+"]
----
cd ../causeway-site

git checkout asf-site
git pull --ff-only
----

* still in the `causeway-site` repo, run the `copyover.sh` script:
+
[source,bash,subs="attributes+"]
----
sh copyover.sh
----
+
This deletes all the files in `content/` _except_ for the `schema` and `versions` directories, and copies the generated Antora site to `causeway-site` repo's `contents` directory:
+
[source,bash,subs="attributes+"]
----
#!/usr/bin/env bash
pushd content
for a in $(ls -1 | grep -v schema | grep -v versions)
do
    rm -rf $a
done
popd

pushd ../causeway
cp -Rf antora/target/site/* ../causeway-site/content/.
popd

git add .
----

* Commit the changes and preview:
+
[source,bash,subs="attributes+"]
----
git commit -m "updates website"

sh preview.sh
----

* If everything looks ok, then push the changes to make live, and switch back to the `causeway` repo:
+
[source,bash,subs="attributes+"]
----
git push origin asf-site
----

[#update-the-algolia-search-index]
== Update the Algolia search index

Create a `algolia.env` file holding the `APP_ID` and the admin `API_KEY`, in the root of `causeway-site`:

[source,ini]
.algolia.env
----
APPLICATION_ID=...
API_KEY=...
----

CAUTION: This file should not be checked into the repo, because the API_KEY allows the index to be modified or deleted.

We use the Algolia-provided link:https://hub.docker.com/r/algolia/docsearch-scraper[docker image] for the crawler to perform the search (as per the link:as per https://docsearch.algolia.com/docs/run-your-own/#run-the-crawl-from-the-docker-image[docs]):

[source,bash]
----
cd content
docker run -it --env-file=../algolia.env -e "CONFIG=$(cat ../algolia-config.json | jq -r tostring)" algolia/docsearch-scraper:v1.16.0
----

This posts the index up to the link:https://algolia.com[Algolia] site.

NOTE: Additional config options for the crawler can be found link:https://www.algolia.com/doc/api-reference/crawler/[here].
